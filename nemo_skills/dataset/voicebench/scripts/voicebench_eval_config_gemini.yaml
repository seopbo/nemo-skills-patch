# VoiceBench evaluation configuration
# Copy and modify this file for your setup

# Cluster settings
cluster: oci_iad
partition: batch_block1,batch_block3,batch_block4
cpu_partition: cpu  # For scoring jobs (no GPU needed)

# Model and server
model: gcp/google/gemini-2.5-pro
server_server_type: vllm  # Model class with audio handling
server_type: openai       # Pipeline routing (CPU partition, external API)
server_gpus: 0            # Not self-hosting, using external API
num_chunks: 1
server_address: https://inference-api.nvidia.com/v1
api_key_env_var: NVIDIA_API_KEY  # Tell VLLMModel which env var has the API key
# server_container: not needed for external API
server_args: ""

# Paths
data_dir: /lustre/fsw/portfolios/llmservice/users/vmendelev/experiments/voicebench_test/data_dir
output_dir: /lustre/fsw/portfolios/llmservice/users/vmendelev/experiments/voicebench_demo/runs/voicebench
voicebench_repo_path: /lustre/fsw/portfolios/llmservice/users/vmendelev/code/VoiceBench

# Subtests to evaluate (list or "all")
# Available: advbench, alpacaeval, alpacaeval_full, alpacaeval_speaker, bbh,
#            commoneval, ifeval, mmsu, mtbench, openbookqa, sd_qa, wildvoice
subtests:
  - sd_qa          # QA with PEDANT + GPT judge

  

# Scoring settings
api_type: nvidia
nvidia_model: azure/openai/gpt-4o-mini

# Installation commands
installation_command: ""  # For generation phase (if needed)
scoring_installation_command: "pip install sacrebleu qa_metrics"  # For scoring phase

# Experiment settings
expname: voicebench
max_samples: 10  # Set to integer for testing (e.g., 10)

# Run modes
generation_only: false
scoring_only: false
dry_run: false

