# VoiceBench full evaluation configuration for S2S session backend
# Run: python generate_from_api_and_score_official.py --config voicebench_s2s_session_full_config.yaml

# Cluster settings
cluster: oci_iad
partition: batch_block1,batch_block3,batch_block4
cpu_partition: cpu  # For scoring jobs (no GPU needed)

# Model and server - using serve_unified with S2S session backend
# dec 12
# model: /lustre/fsw/portfolios/convai/users/ecasanova/Checkpoints/Nemotron-VoiceChat-november/duplex-eartts-2mim_sw_et_eos_dp_eos_dup_fp32-stt-3-december_stt_edresson_model_R_digits_norm_eip_0.1_EA_model_step_9005
# dec 4
# model: /lustre/fsw/portfolios/convai/users/ecasanova/Checkpoints/Nemotron-VoiceChat-november/duplex-eartts-fp32-stt-3-december_stt_edresson_model_R_digits_norm_eip_0.1_EA_model_step_9005
# Nov
model: /lustre/fsw/portfolios/convai/users/ecasanova/Checkpoints/Nemotron-VoiceChat-november/duplex-eartts-fp32-stt-22-november_stt_v2_fp32
server_type: vllm
server_gpus: 1
num_chunks: 1

# Use serve_unified with S2S session backend (no silence padding)
server_entrypoint: "-m nemo_skills.inference.server.serve_unified"
server_args: >-
  --backend s2s_session
  --config_path /lustre/fsw/portfolios/convai/users/ecasanova/S2S-Duplex-new-codebase/scripts/configs/inference/nanov2_demo_model_eartts_updated.yaml
  --speaker_reference /lustre/fsw/portfolios/convai/users/ecasanova/S2S-full-duplex/inference_references/Emma_S3_A1_SC7_singleturntarget_21_channel_1_audio_in.wav
  --ignore_system_prompt
  --num_frames_per_inference 2
  --silence_padding_sec 0.0
  --output_frame_alignment
  --session_artifacts_dir /lustre/fsw/portfolios/llmservice/users/vmendelev/tmp/voicebench_s2s_session_full/artifacts
  --code_path /lustre/fsw/portfolios/convai/users/kevinhu/S2S-Duplex-new-codebase/branches/NeMo-release_not_rebased

# Container with NeMo and S2S support
server_container: /lustre/fsw/portfolios/convai/users/ecasanova/docker_images/nemo_duplex_november_eartts.sqsh

# Use vllm_multimodal to save audio to files instead of base64 in jsonl
server_server_type: vllm_multimodal

# Paths
data_dir: /lustre/fsw/portfolios/llmservice/users/vmendelev/experiments/voicebench_test/data_dir
output_dir: /lustre/fsw/portfolios/llmservice/users/vmendelev/experiments/voicebench_demo/runs/voicebench
voicebench_repo_path: /lustre/fsw/portfolios/llmservice/users/vmendelev/code/VoiceBench

# Subtests to evaluate
#  - openbookqa
#  - mmsu
#  - alpacaeval
#  - commoneval
#  - sd_qa_usa
#  - advbench
subtests:
  - sd_qa_usa

# Scoring settings
api_type: nvidia
nvidia_model: azure/openai/gpt-4o-mini

# Installation commands
installation_command: ""
scoring_installation_command: "pip install sacrebleu qa_metrics"

# Experiment settings
expname: voicebench
max_samples: 10  # Uncomment to limit samples for testing

# Run modes
generation_only: false
scoring_only: false
dry_run: false
