# S2S Demo Evaluation Config
# Usage: python run_s2s_demo_eval.py --config s2s_demo_eval_config.yaml

# Cluster settings
cluster: oci_iad
partition: batch_block1,batch_block3,batch_block4

# Model and server
# model: /lustre/fsw/portfolios/convai/users/ecasanova/Checkpoints/Nemotron-VoiceChat-november/duplex-eartts-fp32-stt-22-november_stt_v2_fp32
model: /lustre/fsw/portfolios/convai/users/ecasanova/Checkpoints/Nemotron-VoiceChat-november/duplex-eartts-2mim_sw_et_eos_dp_eos_dup_fp32-stt-3-december_stt_edresson_model_R_digits_norm_eip_0.1_EA_model_step_9005
server_type: vllm
server_gpus: 1
server_entrypoint: "-m nemo_skills.inference.server.serve_unified"
server_args: >-
  --backend s2s_incremental
  --config_path /lustre/fsw/portfolios/convai/users/ecasanova/S2S-Duplex-new-codebase/scripts/configs/inference/nanov2_demo_model_eartts_updated.yaml
  --speaker_reference /lustre/fsw/portfolios/convai/users/ecasanova/S2S-full-duplex/inference_references/Emma_S3_A1_SC7_singleturntarget_21_channel_1_audio_in.wav
  --ignore_system_prompt
  --num_frames_per_inference 2
  --silence_padding_sec 0.0
  --output_frame_alignment
  --session_artifacts_dir /lustre/fsw/portfolios/llmservice/users/vmendelev/tmp/s2s_demo_eval/artifacts
  --code_path /lustre/fsw/portfolios/convai/users/kevinhu/S2S-Duplex-new-codebase/branches/NeMo-release_not_rebased
server_container: /lustre/fsw/portfolios/convai/users/ecasanova/docker_images/nemo_duplex_november_eartts.sqsh

# Hydra overrides for generation
server_server_type: vllm_multimodal

# Data and output
data_dir: /lustre/fsw/portfolios/llmservice/users/vmendelev/experiments/voicebench_test/data_dir
benchmark: s2s_demo.demo_20251124
output_dir: /lustre/fsw/portfolios/llmservice/users/vmendelev/tmp/s2s_demo_eval_t4_c2

# Job settings
expname: s2s_demo_eval
num_chunks: 8
# max_samples: 2  # Uncomment to limit samples for testing

# Pipeline control (can also be set via CLI flags)
# generation_only: false  # Only run generation, skip scoring
# scoring_only: false     # Only run scoring, skip generation
# llm_judge_only: false   # Only run LLM judge, skip generation and scoring

# Scoring settings
# scoring_container: gitlab-master.nvidia.com/pzelasko/nemo_containers:25.04-pytorch2.7-28may25
scoring_container: /lustre/fsw/portfolios/llmservice/users/pzelasko/containers/nemo-25.04-pytorch2.7-28may25.sqsh
scoring_partition: batch_block1,batch_block3,batch_block4

scoring:
  barge_in_threshold_sec: 1.5
  tt_latency_threshold_sec: 1.5
  tt_precision_buffer_sec: 0.5
  tt_recall_buffer_sec: 0.5
  vad_min_silence_duration_ms: 1500
  verbose: true
  disable_transcription: false
  save_per_sample_results: true  # Saves segmentation/ASR to output_with_eval.jsonl
  force_recompute: false  # Recompute segmentation/ASR even if cached
  segment_buffer_sec: 1

# LLM Judge settings
llm_judge:
  enabled: true
  model: us/azure/openai/gpt-4.1
  server_type: openai
  base_url: https://inference-api.nvidia.com/v1
