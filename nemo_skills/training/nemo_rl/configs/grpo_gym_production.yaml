# Production GRPO-Gym Configuration
# Based on tested grpo_gym.yaml, optimized for cluster training

grpo:
  max_num_epochs: 1
  num_prompts_per_step: 512
  num_generations_per_prompt: 16
  max_rollout_turns: 1
  max_num_steps: 1000
  normalize_rewards: true
  use_leave_one_out_baseline: true
  val_period: 50
  val_at_start: true
  overlong_filtering: false
  max_val_samples: 100
  val_batch_size: null
  seed: 42
  use_dynamic_sampling: false
  skip_reference_policy_logprobs_calculation: true

loss_fn:
  reference_policy_kl_penalty: 0.01
  reference_policy_kl_type: "k3"
  kl_input_clamp_value: 20.0
  kl_output_clamp_value: 10.0
  truncated_importance_sampling_ratio: null
  force_on_policy_ratio: false
  token_level_loss: true

checkpointing:
  enabled: true
  checkpoint_dir: "results/grpo_gym_production"
  metric_name: "val:reward"
  higher_is_better: true
  keep_top_k: 5
  save_period: 50
  checkpoint_must_save_by: null

policy:
  model_name: "Qwen/Qwen2.5-Math-7B-Instruct"
  tokenizer:
    name: ${policy.model_name}
    chat_template_kwargs: null
  train_global_batch_size: ${mul:${grpo.num_prompts_per_step}, ${grpo.num_generations_per_prompt}}
  train_micro_batch_size: 4
  generation_batch_size: 32
  logprob_batch_size: 4
  max_total_sequence_length: 4096
  precision: "bfloat16"
  fsdp_offload_enabled: false
  activation_checkpointing_enabled: true
  refit_buffer_size_gb: 8
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  context_parallel_size: 1
  lr: 5e-7
  weight_decay: 0.01
  min_lr: 5e-8
  offload_optimizer_for_logprob: false

  dtensor_cfg:
    enabled: true
    cpu_offload: false
    sequence_parallel: false
    activation_checkpointing: true
    tensor_parallel_size: ${policy.tensor_model_parallel_size}
    context_parallel_size: ${policy.context_parallel_size}

  sequence_packing:
    enabled: false

  make_sequence_length_divisible_by: null
  max_grad_norm: 1.0

  optimizer:
    name: "torch.optim.AdamW"
    kwargs:
      lr: ${policy.lr}
      weight_decay: ${policy.weight_decay}
      betas: [0.9, 0.999]
      eps: 1e-8
      foreach: false
      fused: false

  scheduler:
  - name: "torch.optim.lr_scheduler.LinearLR"
    kwargs:
      start_factor: 1.0
      end_factor: 1.0
      total_iters: 1
  - name: "torch.optim.lr_scheduler.CosineAnnealingLR"
    kwargs:
      T_max: ${grpo.max_num_steps}
      eta_min: ${policy.min_lr}
  - milestones: [0]

  megatron_cfg:
    enabled: false

  generation:
    backend: "vllm"
    max_new_tokens: ${policy.max_total_sequence_length}
    temperature: 1.0
    top_p: 1.0
    top_k: null
    stop_token_ids: null
    stop_strings: null
    vllm_cfg:
      async_engine: true
      expose_http_server: true
      skip_tokenizer_init: false  # Required: Enable tokenizer for NemoGym chat completions API
      precision: ${policy.precision}
      tensor_parallel_size: 1
      pipeline_parallel_size: 1
      expert_parallel_size: 1
      gpu_memory_utilization: 0.8
      max_model_len: ${policy.max_total_sequence_length}
      enable_expert_parallel: false
      enforce_eager: false
      load_format: auto
      kv_cache_dtype: auto
    colocated:
      enabled: true
      resources:
        gpus_per_node: null
        num_nodes: null

data:
  max_input_seq_length: ${policy.max_total_sequence_length}
  shuffle: true
  num_workers: 4
  train_jsonl_fpath: null
  validation_jsonl_fpath: null

env:
  should_use_nemo_gym: true
  should_log_nemo_gym_responses: true
  nemo_gym:
    config_paths:
    - responses_api_models/vllm_model/configs/vllm_model_for_training.yaml
    - resources_servers/math_with_judge/configs/math_with_judge.yaml
    - responses_api_agents/simple_agent/configs/simple_agent.yaml
    policy_model:
      responses_api_models:
        vllm_model:
          skip_tokenizer_init: false
          return_token_id_information: true  # Required: Return token IDs for training
    simple_agent:
      responses_api_agents:
        simple_agent:
          resources_server:
            name: math_with_judge
          model_server:
            name: policy_model
    math_with_judge:
      resources_servers:
        math_with_judge:
          judge_model_server:
            name: policy_model
          should_use_judge: false  # Optional: Enable LLM judge for answer verification

logger:
  log_dir: "logs/grpo_gym_production"
  num_val_samples_to_print: 5
  wandb_enabled: true
  tensorboard_enabled: false
  mlflow_enabled: false
  swanlab_enabled: false
  monitor_gpus: true
  wandb:
    project: "nemo-skills-grpo-gym"
    name: null
  gpu_monitoring:
    collection_interval: 10
    flush_interval: 10

cluster:
  gpus_per_node: 8
  num_nodes: 1
