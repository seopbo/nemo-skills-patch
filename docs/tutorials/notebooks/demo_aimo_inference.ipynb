{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62aa155e-9613-4ca9-a88d-d11ce4ac8b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igitman/workspace/NeMo-Skills/venv-aimo3/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX A6000 \n",
      "GPU 1: NVIDIA RTX A6000 \n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers.utils import logging\n",
    "\n",
    "from nemo_skills.inference.model import get_tool_calling_model\n",
    "from nemo_skills.inference.model.base import EndpointType\n",
    "from nemo_skills.prompt.utils import get_prompt\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "!nvidia-smi -L | cut -d '(' -f 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93801d3c-3388-4d2d-982a-031b39d0bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(bench, tokenizer):\n",
    "    data = {\n",
    "        \"Metric\": [\n",
    "            \"Num Generations\",\n",
    "            \"Total Generation Time\",\n",
    "            \"Batch Throughput (Tok/sec)\",\n",
    "            \"Avg Request Throughput (Tok/sec)\",\n",
    "        ]\n",
    "    }\n",
    "    for name, rec in bench.items():\n",
    "        gens = [g for g in rec[\"gens\"] if isinstance(g, str)]\n",
    "        tt = max(rec[\"total_time\"], 1e-9)\n",
    "        toks = np.array([len(tokenizer.encode(g)) for g in gens], dtype=float)\n",
    "        batch_tp = toks.sum() / tt\n",
    "        per_req_tp = toks / tt\n",
    "        data[name] = [\n",
    "            f\"{len(gens)}\",\n",
    "            f\"{tt:.1f}\",\n",
    "            f\"{batch_tp:.1f}\",\n",
    "            f\"{per_req_tp.mean():.1f} Â± {per_req_tp.std():.1f}\",\n",
    "        ]\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cdc825f-d200-48d6-8d61-ae6d83a7a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_obj = get_prompt(\"generic/math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9d9651-4da8-4ea5-9817-0cf1cba21b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
    "\n",
    "llm = get_tool_calling_model(\n",
    "    model,\n",
    "    server_type=\"vllm\",\n",
    "    tool_modules=[\"nemo_skills.mcp.servers.python_tool::PythonTool\"],\n",
    "    additional_config={\"sandbox\": {}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5765fca5-84d5-4748-b7c9-12a4e59354a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = r\"\"\"\n",
    "The Fibonacci numbers are defined as follows: $F_0 = 0$, $F_1 = 1$, and $F_{n+1} = F_n + F_{n-1}$ for $n \\geq 1$.\n",
    "There are $N$ positive integers $n$ strictly less than $10^{101}$ such that $n^2 + (n+1)^2$ is a multiple of 5 but $F_{n-1}^2 + F_n^2$ is not.\n",
    "How many prime factors does $N$ have, counted with multiplicity?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad8a8b07-35bf-4a70-a8e6-66a9f21193cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df130bdc-d05d-4264-8d28-a44cb6c9e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm done! 3\n",
      "I'm cancelled! 12\n",
      "I'm cancelled! 4\n",
      "I'm cancelled! 1\n",
      "I'm cancelled! 8\n",
      "I'm cancelled! 18\n",
      "I'm cancelled! 11\n",
      "I'm cancelled! 9\n",
      "I'm cancelled! 0\n",
      "I'm cancelled! 2\n",
      "I'm cancelled! 6\n",
      "I'm cancelled! 19\n",
      "I'm cancelled! 5\n",
      "I'm cancelled! 17\n",
      "I'm cancelled! 10\n",
      "I'm cancelled! 13\n",
      "I'm cancelled! 14\n",
      "I'm cancelled! 15\n",
      "I'm cancelled! 16\n",
      "I'm cancelled! 7\n"
     ]
    }
   ],
   "source": [
    "async def run_generation(request_id, prompt_obj, llm, problem):\n",
    "    stream = None\n",
    "    full_generation = \"\"\n",
    "    try:\n",
    "        stream = await llm.generate_async(\n",
    "            prompt=prompt_obj.fill({\"problem\": problem}),\n",
    "            endpoint_type=EndpointType.chat,\n",
    "            stream=True,\n",
    "            random_seed=request_id,\n",
    "            temperature=1.0,\n",
    "            tokens_to_generate=5000,\n",
    "        )\n",
    "        async for response in stream:\n",
    "            full_generation += response[\"generation\"]\n",
    "        print(\"I'm done!\", request_id)\n",
    "        return {\"request_id\": request_id, \"status\": \"complete\", \"generation\": full_generation}\n",
    "    except asyncio.CancelledError:\n",
    "        if stream is not None:\n",
    "            await stream.aclose()\n",
    "        print(\"I'm cancelled!\", request_id)\n",
    "        return {\"request_id\": request_id, \"status\": \"partial\", \"generation\": full_generation}\n",
    "    except Exception as e:\n",
    "        if stream is not None:\n",
    "            await stream.aclose()\n",
    "        print(f\"Generation {request_id} failed with error: {e}\")\n",
    "        return {\"request_id\": request_id, \"status\": \"error\", \"generation\": full_generation}\n",
    "\n",
    "\n",
    "async def main_loop(prompt_obj, llm, problem):\n",
    "    num_generations = 20\n",
    "    cancel_after_done = 1\n",
    "\n",
    "    completed = 0\n",
    "    complete_generations = []\n",
    "    partial_generations = []\n",
    "\n",
    "    tasks = [asyncio.create_task(run_generation(i, prompt_obj, llm, problem)) for i in range(num_generations)]\n",
    "    pending = set(tasks)\n",
    "\n",
    "    while pending:\n",
    "        done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)\n",
    "        for task in done:\n",
    "            result = task.result()\n",
    "            if result[\"status\"] == \"complete\":\n",
    "                complete_generations.append(result[\"generation\"])\n",
    "                completed += 1\n",
    "            else:\n",
    "                partial_generations.append(result[\"generation\"])\n",
    "\n",
    "        if completed >= cancel_after_done and pending:\n",
    "            for task in pending:\n",
    "                task.cancel()\n",
    "\n",
    "    return complete_generations, partial_generations\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "# Suppress tool-call cancellation noise from anyio TaskGroup shutdowns.\n",
    "try:\n",
    "    import anyio\n",
    "except Exception:\n",
    "    anyio = None\n",
    "\n",
    "\n",
    "class _ToolCallCancelFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        exc = record.exc_info[1] if record.exc_info else None\n",
    "        if exc is None:\n",
    "            return True\n",
    "        if isinstance(exc, asyncio.CancelledError):\n",
    "            return False\n",
    "        if isinstance(exc, BaseExceptionGroup):\n",
    "            for sub_exc in exc.exceptions:\n",
    "                if isinstance(sub_exc, asyncio.CancelledError):\n",
    "                    return False\n",
    "                if anyio and isinstance(sub_exc, (anyio.BrokenResourceError, anyio.ClosedResourceError)):\n",
    "                    return False\n",
    "            return True\n",
    "        if anyio and isinstance(exc, (anyio.BrokenResourceError, anyio.ClosedResourceError)):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "class _ProcessKillNoiseFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        if record.name != \"mcp.os.posix.utilities\":\n",
    "            return True\n",
    "        if not record.getMessage().startswith(\"Failed to kill process\"):\n",
    "            return True\n",
    "        exc = record.exc_info[1] if record.exc_info else None\n",
    "        if exc is None:\n",
    "            return False\n",
    "        if isinstance(exc, ProcessLookupError):\n",
    "            return False\n",
    "        if isinstance(exc, BaseExceptionGroup):\n",
    "            return not any(isinstance(sub_exc, ProcessLookupError) for sub_exc in exc.exceptions)\n",
    "        return True\n",
    "\n",
    "\n",
    "tool_logger = logging.getLogger(\"nemo_skills.inference.model.tool_call\")\n",
    "tool_logger.addFilter(_ToolCallCancelFilter())\n",
    "\n",
    "posix_logger = logging.getLogger(\"mcp.os.posix.utilities\")\n",
    "posix_logger.addFilter(_ProcessKillNoiseFilter())\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "complete_generations, partial_generations = await main_loop(prompt_obj, llm, problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da901d45-bb3a-42e2-9b1d-41f2fee11757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete generations token counts:\n",
      "  complete[0]: 5000\n",
      "\n",
      "Partial generations token counts:\n",
      "  partial[0]: 4410\n",
      "  partial[1]: 4751\n",
      "  partial[2]: 4666\n",
      "  partial[3]: 3898\n",
      "  partial[4]: 3988\n",
      "  partial[5]: 2722\n",
      "  partial[6]: 3824\n",
      "  partial[7]: 4998\n",
      "  partial[8]: 4997\n",
      "  partial[9]: 4761\n",
      "  partial[10]: 3769\n",
      "  partial[11]: 4607\n",
      "  partial[12]: 3611\n",
      "  partial[13]: 4702\n",
      "  partial[14]: 4998\n",
      "  partial[15]: 4251\n",
      "  partial[16]: 4496\n",
      "  partial[17]: 4430\n",
      "  partial[18]: 4536\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    tokenizer = llm.tokenizer\n",
    "except Exception:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "\n",
    "\n",
    "def _tok_count(text: str) -> int:\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "print(\"Complete generations token counts:\")\n",
    "for i, g in enumerate(complete_generations):\n",
    "    print(f\"  complete[{i}]: {_tok_count(g)}\")\n",
    "\n",
    "print(\"\\nPartial generations token counts:\")\n",
    "for i, g in enumerate(partial_generations):\n",
    "    print(f\"  partial[{i}]: {_tok_count(g)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238b2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-aimo3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
