# Cluster config for Judge test - only uses GPU 1
# GPU 0 is reserved for external Judge vLLM

executor: local

containers:
  trtllm: nvcr.io/nvidia/tensorrt-llm/release:1.0.0
  vllm: vllm/vllm-openai:v0.10.2
  sglang: lmsysorg/sglang:v0.5.4
  sandbox: dockerfile:dockerfiles/Dockerfile.sandbox
  nemo-skills: dockerfile:dockerfiles/Dockerfile.nemo-skills
  megatron: dockerfile:dockerfiles/Dockerfile.megatron
  verl: dockerfile:dockerfiles/Dockerfile.verl
  # Use the same reproducible nemo-rl image build as other local GPU tests to
  # avoid dependency drift inside Ray isolated worker venvs (e.g., numpy major mismatch).
  nemo-rl: dockerfile:dockerfiles/Dockerfile.nemo-rl

mounts:
  - /tmp:/tmp
  # Only mount specific directories, NOT entire home (which contains ~/.cache/uv)
  - /home/binhu/nemo-skills-tests:/home/binhu/nemo-skills-tests
  - /home/binhu/code/hf_models:/root/.cache/huggingface
  # change this if the models are located in a different place
  - /mnt/datadrive/nemo-skills-test-data:/mnt/datadrive/nemo-skills-test-data

env_vars:
  - HF_HOME=/root/.cache/huggingface
  - CUDA_VISIBLE_DEVICES=1
  # Put NeMo-RL Ray isolated worker venvs on a mounted path we can clean between runs.
  # This avoids stale venv reuse inside docker_persistent containers (e.g., numpy major mismatch).
  - NEMO_RL_VENV_DIR=/mnt/datadrive/nemo-skills-test-data/hf-cache/nemo_rl/ray_venvs