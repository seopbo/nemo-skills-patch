# Phase 1 Experiment: Compare two models on a small sample
# Purpose: Validate if dual-model validation is worth the cost

cluster: slurm
base_output_dir: /workspace/rl-data-clean/experiment
expname: model-comparison-experiment
suffix: phase1

# Small sample for experiment (1000 problems)
input_file: ${base_output_dir}/raw_aops_sample_1000.jsonl

# Model configurations
model_120b:
  model: gpt-oss-120B
  server_type: trtllm
  server_gpus: 8
  server_nodes: 2
  num_chunks: 5

model_deepseek:
  model: deepseek-v3
  server_type: trtllm
  server_gpus: 8
  server_nodes: 2
  num_chunks: 5

# Simplified pipeline for experiment
# Focus on comparing the two most critical stages
pipeline_stages:
  - extract_problems          # Use 120B as baseline
  - classify_if_proof         # Use 120B as baseline

  # Run proof quality assessment with BOTH models (in parallel)
  - assess_proof_quality_120b
  - assess_proof_quality_deepseek

  # Compare results
  - compare_proof_quality

  # Run IMO readiness with BOTH models (in parallel)
  - assess_imo_readiness_120b
  - assess_imo_readiness_deepseek

  # Compare results
  - compare_imo_readiness

directories:
  step-1-extract: ${base_output_dir}/step-1-extract
  step-2-classify: ${base_output_dir}/step-2-classify
  step-3-proof-120b: ${base_output_dir}/step-3-proof-quality-120b
  step-3-proof-deepseek: ${base_output_dir}/step-3-proof-quality-deepseek
  step-4-comparison: ${base_output_dir}/step-4-comparison
  step-5-imo-120b: ${base_output_dir}/step-5-imo-readiness-120b
  step-5-imo-deepseek: ${base_output_dir}/step-5-imo-readiness-deepseek
  step-6-comparison: ${base_output_dir}/step-6-comparison

stages:
  extract_problems:
    output_dir: ${directories.step-1-extract}
    input_file: ${input_file}
    dependencies: null
    inline_args: ""
    stage_kwargs: ${model_120b}

  classify_if_proof:
    output_dir: ${directories.step-2-classify}
    input_file: ${directories.step-1-extract}/extracted-problems.jsonl
    dependencies:
      - extract_problems
    inline_args: ""
    stage_kwargs: ${model_120b}

  # Proof quality assessment - Model 1 (120B)
  assess_proof_quality_120b:
    output_dir: ${directories.step-3-proof-120b}
    input_file: ${directories.step-2-classify}/proof.jsonl
    dependencies:
      - classify_if_proof
    inline_args: "++prompt_config=/nemo_run/code/recipes/rl-data-clean/prompts/assess-proof-quality.yaml"
    stage_kwargs: ${model_120b}

  # Proof quality assessment - Model 2 (DeepSeek)
  assess_proof_quality_deepseek:
    output_dir: ${directories.step-3-proof-deepseek}
    input_file: ${directories.step-2-classify}/proof.jsonl
    dependencies:
      - classify_if_proof
    inline_args: "++prompt_config=/nemo_run/code/recipes/rl-data-clean/prompts/assess-proof-quality.yaml"
    stage_kwargs: ${model_deepseek}

  # Compare proof quality assessments
  compare_proof_quality:
    output_dir: ${directories.step-4-comparison}
    dependencies:
      - assess_proof_quality_120b
      - assess_proof_quality_deepseek
    # This will be a script execution, not a generate() call
    script: |
      python /nemo_run/code/recipes/rl-data-clean/scripts/compare_model_outputs.py \
        ${directories.step-3-proof-120b}/output.jsonl \
        ${directories.step-3-proof-deepseek}/output.jsonl \
        ${directories.step-4-comparison}/proof_quality_comparison.json \
        --stage proof_quality \
        --model1-name "gpt-oss-120B" \
        --model2-name "deepseek-v3"

  # IMO readiness - Model 1 (120B)
  assess_imo_readiness_120b:
    output_dir: ${directories.step-5-imo-120b}
    input_file: ${directories.step-3-proof-120b}/output.jsonl
    dependencies:
      - assess_proof_quality_120b
    inline_args: "++prompt_config=/nemo_run/code/recipes/rl-data-clean/prompts/assess-imo-readiness.yaml"
    stage_kwargs: ${model_120b}

  # IMO readiness - Model 2 (DeepSeek)
  assess_imo_readiness_deepseek:
    output_dir: ${directories.step-5-imo-deepseek}
    input_file: ${directories.step-3-proof-deepseek}/output.jsonl
    dependencies:
      - assess_proof_quality_deepseek
    inline_args: "++prompt_config=/nemo_run/code/recipes/rl-data-clean/prompts/assess-imo-readiness.yaml"
    stage_kwargs: ${model_deepseek}

  # Compare IMO readiness assessments
  compare_imo_readiness:
    output_dir: ${directories.step-6-comparison}
    dependencies:
      - assess_imo_readiness_120b
      - assess_imo_readiness_deepseek
    script: |
      python /nemo_run/code/recipes/rl-data-clean/scripts/compare_model_outputs.py \
        ${directories.step-5-imo-120b}/output.jsonl \
        ${directories.step-5-imo-deepseek}/output.jsonl \
        ${directories.step-6-comparison}/imo_readiness_comparison.json \
        --stage imo_readiness \
        --model1-name "gpt-oss-120B" \
        --model2-name "deepseek-v3"
