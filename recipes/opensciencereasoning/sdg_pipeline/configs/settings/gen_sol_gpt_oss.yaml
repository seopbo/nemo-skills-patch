# Generate solutions stage configuration
# This can be included in pipeline configs using !include
stages:
  generate_solutions:
    make_majority_voting: False
    output_dir: ${directories.step-4-generate-solutions}
    predicted_answer_regex: null
    predicted_answer_regex_field: null # if specified will use the field in the input file to extract the predicted answer regex
    make_judgement: True

    reasoning_effort: ++chat_template_kwargs.reasoning_effort=high
    prompt_config: ++prompt_config=generic/general-boxed
    code_tags: ++code_tags=gpt-oss
    enable_code_execution: ++code_execution=true
    code_execution_limits: ++server.code_execution.max_code_executions=100
    builtin_tools: ++chat_template_kwargs.builtin_tools=[python]
    parse_reasoning: ++parse_reasoning=true
    end_reasoning_string: '''++end_reasoning_string="<|start|>assistant<|channel|>final<|message|>"'''
    generation_kwargs:
      args:
        input_file: ${directories.step-1-decontaminate}/final_result.jsonl
        model: /hf_models/gpt-oss-120b
        server_type: vllm
        server_gpus: 8
        server_nodes: 1
        dependent_jobs: 1
        num_chunks: 20
        num_random_seeds: 5
        server_args: "--async-scheduling"
        with_sandbox: true

      ctx_args: >-
        ${stages.generate_solutions.prompt_config}
        ${stages.generate_solutions.code_tags}
        ${stages.generate_solutions.enable_code_execution}
        ${stages.generate_solutions.code_execution_limits}
        ++max_concurrent_requests=1024
        ++inference.tokens_to_generate=80_000
        ++inference.endpoint_type=text
        ++inference.temperature=1.0
        ++inference.top_p=1.0
        ${stages.generate_solutions.reasoning_effort}
        ${stages.generate_solutions.builtin_tools}
        ${stages.generate_solutions.parse_reasoning}
        ${stages.generate_solutions.end_reasoning_string}
