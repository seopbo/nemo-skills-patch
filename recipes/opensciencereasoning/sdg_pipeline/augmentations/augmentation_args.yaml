# Base configuration for MCQ augmentation
cluster: null  # Will be filled via command line
output_dir: null  # Will be filled via command line  
input_file: null  # Will be filled via command line

generation_kwargs:
  args:
    model: /hf_models/gpt-oss-20b
    server_type: vllm
    server_gpus: 8
    server_nodes: 1
    dependent_jobs: 1
    num_chunks: 20
  ctx_args: >-
    ++prompt_config=/nemo_run/code/recipes/opensciencereasoning/sdg_pipeline/augmentations/augment_mcq.yaml
    ++inference.tokens_to_generate=16000
    ++inference.temperature=1.0
    ++inference.top_p=1.0
    ++chat_template_kwargs.reasoning_effort=high
