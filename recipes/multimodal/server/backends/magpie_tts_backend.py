# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0

"""MagpieTTS backend using MagpieInferenceRunner with RTF metrics."""

import io
import json
import os
import shutil
import tempfile
import time
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Set

import soundfile as sf

from .base import BackendConfig, GenerationRequest, GenerationResult, InferenceBackend, Modality


@dataclass
class MagpieTTSConfig(BackendConfig):
    codec_model_path: Optional[str] = None
    top_k: int = 80
    temperature: float = 0.6
    use_cfg: bool = True
    cfg_scale: float = 2.5
    max_decoder_steps: int = 440
    use_local_transformer: bool = False
    output_sample_rate: int = 22050

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "MagpieTTSConfig":
        known = {
            "model_path",
            "device",
            "dtype",
            "max_new_tokens",
            "temperature",
            "top_p",
            "top_k",
            "codec_model_path",
            "use_cfg",
            "cfg_scale",
            "max_decoder_steps",
            "use_local_transformer",
            "output_sample_rate",
        }
        return cls(
            **{k: v for k, v in d.items() if k in known}, extra_config={k: v for k, v in d.items() if k not in known}
        )


class MagpieTTSBackend(InferenceBackend):
    """MagpieTTS backend. Input: JSON with 'text' and 'context_audio_filepath'."""

    @property
    def name(self) -> str:
        return "magpie_tts"

    @property
    def supported_modalities(self) -> Set[Modality]:
        return {Modality.TEXT, Modality.AUDIO_OUT}

    def __init__(self, config: BackendConfig):
        self.tts_config = (
            config
            if isinstance(config, MagpieTTSConfig)
            else MagpieTTSConfig.from_dict(
                {
                    **{
                        k: getattr(config, k)
                        for k in ["model_path", "device", "dtype", "max_new_tokens", "temperature", "top_p", "top_k"]
                        if hasattr(config, k)
                    },
                    **config.extra_config,
                }
            )
        )
        super().__init__(self.tts_config)
        self._model = self._runner = self._temp_dir = self._checkpoint_name = None

    def load_model(self) -> None:
        from nemo.collections.tts.modules.magpietts_inference.inference import InferenceConfig, MagpieInferenceRunner
        from nemo.collections.tts.modules.magpietts_inference.utils import ModelLoadConfig, load_magpie_model

        if not self.tts_config.codec_model_path:
            raise ValueError("codec_model_path required")

        model_path = self.config.model_path
        cfg = ModelLoadConfig(nemo_file=model_path, codecmodel_path=self.tts_config.codec_model_path)
        self._model, self._checkpoint_name = load_magpie_model(cfg, device=self.config.device)

        self._runner = MagpieInferenceRunner(
            self._model,
            InferenceConfig(
                temperature=self.tts_config.temperature,
                topk=self.tts_config.top_k,
                max_decoder_steps=self.tts_config.max_decoder_steps,
                use_cfg=self.tts_config.use_cfg,
                cfg_scale=self.tts_config.cfg_scale,
                use_local_transformer=self.tts_config.use_local_transformer,
                batch_size=32,
            ),
        )

        self._temp_dir = tempfile.mkdtemp(prefix="magpie_tts_")
        self.tts_config.output_sample_rate = self._model.sample_rate
        self._is_loaded = True
        print(
            f"[MagpieTTSBackend] Loaded: {self._checkpoint_name}, sr={self._model.sample_rate}, cfg={self.tts_config.use_cfg}"
        )

    def _extract_json(self, text: str) -> dict:
        """Extract JSON object from text, skipping non-JSON parts."""
        if not text:
            return {"text": ""}
        # Find first { and try to parse from there
        idx = text.find("{")
        if idx >= 0:
            try:
                return json.loads(text[idx:])
            except json.JSONDecodeError:
                pass
        return {"text": text}

    def generate(self, requests: List[GenerationRequest]) -> List[GenerationResult]:
        if not self._is_loaded:
            return [GenerationResult(error="Model not loaded", request_id=r.request_id) for r in requests]
        if not requests:
            return []

        start_time = time.time()
        batch_dir = os.path.join(self._temp_dir, f"batch_{int(time.time() * 1000)}")
        output_dir = os.path.join(batch_dir, "output")
        os.makedirs(output_dir, exist_ok=True)

        try:
            # Parse requests, extracting JSON from text (skips non-JSON prefixes)
            parsed = [self._extract_json(r.text) for r in requests]

            # Create audio_dir with symlinks to all context audio files (they may be in different dirs)
            audio_dir = os.path.join(batch_dir, "audio")
            os.makedirs(audio_dir, exist_ok=True)

            manifest_path = os.path.join(batch_dir, "manifest.json")
            with open(manifest_path, "w") as f:
                for i, p in enumerate(parsed):
                    ctx = p.get("context_audio_filepath", "")
                    if ctx and os.path.exists(ctx):
                        # Create unique symlink name to avoid collisions
                        link_name = f"ctx_{i}_{os.path.basename(ctx)}"
                        link_path = os.path.join(audio_dir, link_name)
                        if not os.path.exists(link_path):
                            os.symlink(ctx, link_path)
                    else:
                        link_name = f"d{i}.wav"
                    f.write(
                        json.dumps(
                            {
                                "text": p.get("text", ""),
                                "audio_filepath": link_name,
                                "context_audio_filepath": link_name,
                                "duration": p.get("duration", 5.0),
                                "context_audio_duration": p.get("context_audio_duration", 5.0),
                            }
                        )
                        + "\n"
                    )

            config_path = os.path.join(batch_dir, "config.json")
            with open(config_path, "w") as f:
                json.dump({"batch": {"manifest_path": manifest_path, "audio_dir": audio_dir}}, f)

            # Run inference
            from nemo.collections.tts.modules.magpietts_inference.evaluate_generated_audio import load_evalset_config

            dataset = self._runner.create_dataset(load_evalset_config(config_path))
            rtf_list, _ = self._runner.run_inference_on_dataset(
                dataset, output_dir, save_cross_attention_maps=False, save_context_audio=False
            )

            gen_time = time.time() - start_time
            batch_metrics = {
                "total_time_sec": gen_time,
                "num_samples": len(requests),
                **self._runner.compute_mean_rtf_metrics(rtf_list),
            }

            # Build results
            results = []
            for i, req in enumerate(requests):
                path = os.path.join(output_dir, f"predicted_audio_{i}.wav")
                if os.path.exists(path):
                    audio, sr = sf.read(path)
                    buf = io.BytesIO()
                    sf.write(buf, audio, sr, format="WAV")
                    buf.seek(0)
                    dur = len(audio) / sr
                    results.append(
                        GenerationResult(
                            text=parsed[i].get("text", ""),
                            audio_bytes=buf.read(),
                            audio_sample_rate=self.tts_config.output_sample_rate,
                            audio_format="wav",
                            request_id=req.request_id,
                            generation_time_ms=gen_time * 1000 / len(requests),
                            debug_info={
                                "checkpoint": self._checkpoint_name,
                                "audio_duration_sec": dur,
                                "rtf": gen_time / len(requests) / dur if dur else 0,
                                "config": {
                                    "temp": self.tts_config.temperature,
                                    "top_k": self.tts_config.top_k,
                                    "cfg": self.tts_config.use_cfg,
                                    "cfg_scale": self.tts_config.cfg_scale,
                                },
                                "batch_metrics": batch_metrics,
                            },
                        )
                    )
                else:
                    results.append(GenerationResult(error=f"Audio not found: {path}", request_id=req.request_id))
            return results
        except Exception as e:
            import traceback

            traceback.print_exc()
            return [GenerationResult(error=str(e), request_id=r.request_id) for r in requests]
        finally:
            shutil.rmtree(batch_dir, ignore_errors=True)

    def validate_request(self, request: GenerationRequest) -> Optional[str]:
        return "Text required" if not request.text else None

    def health_check(self) -> Dict[str, Any]:
        h = super().health_check()
        if self._is_loaded:
            h.update(
                {
                    "checkpoint": self._checkpoint_name,
                    "codec": self.tts_config.codec_model_path,
                    "cfg": self.tts_config.use_cfg,
                    "cfg_scale": self.tts_config.cfg_scale,
                    "sample_rate": self.tts_config.output_sample_rate,
                }
            )
        return h

    def __del__(self):
        if getattr(self, "_temp_dir", None) and os.path.exists(self._temp_dir):
            shutil.rmtree(self._temp_dir, ignore_errors=True)
